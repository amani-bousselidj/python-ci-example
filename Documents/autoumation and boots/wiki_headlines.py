import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime

try:
    # Ù†Ø¶ÙŠÙ Ù‡ÙˆÙŠØ© Ø§Ù„Ù…ØªØµÙØ­ Ù„ØªØ¬Ù†Ø¨ Ø±ÙØ¶ Ø§Ù„Ø·Ù„Ø¨
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/118.0.0.0 Safari/537.36"
    }

    url = "https://en.wikipedia.org/wiki/Main_Page"
    response = requests.get(url, headers=headers, timeout=10)

    if response.status_code != 200:
        print(f"âŒ Error: Failed to fetch page (status code {response.status_code})")
        exit()

    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
    soup = BeautifulSoup(response.text, "html.parser")

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙˆÙ„ 5 Ø¹Ù†Ø§ÙˆÙŠÙ† Ù…Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    headlines = [a.get_text(strip=True) for a in soup.select("#mp-upper ul li a")[:5]]

    if not headlines:
        print("âš ï¸ No headlines found on Wikipedia main page.")
        exit()

    # Ø­ÙØ¸Ù‡Ø§ ÙÙŠ JSON Ù…Ø¹ Ø§Ù„ØªÙˆÙ‚ÙŠØª
    data = {
        "timestamp": datetime.now().isoformat(),
        "headlines": headlines
    }

    with open("wiki_headlines.json", "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

    print(f"âœ… Saved {len(headlines)} headlines:")
    print(headlines)

except requests.exceptions.RequestException as e:
    print(f"ğŸŒ Network error: {e}")
except Exception as e:
    print(f"âš ï¸ Unexpected error: {e}")
